Hugging Faceï¼šDatasetsï¼ˆæ•°æ®é›†ï¼‰+ Modelï¼ˆé¢„è®­ç»ƒçš„ ML æ¨¡å‹ï¼‰+ Spacesï¼ˆå…è®¸åœ¨ä¸€ä¸ªæœ‰ç»„ç»‡çš„å·¥ä½œåŒºä¸­æ„å»ºã€æ‰˜ç®¡å’Œå…±äº«æ‚¨çš„ ML åº”ç”¨ç¨‹åºï¼‰
`Transformers` åº“æä¾›äº†ç”¨äºä¸‹è½½ã€è¿è¡Œå’Œè®­ç»ƒæœ€å…ˆè¿›çš„å¼€æº AI æ¨¡å‹çš„ API å’Œå·¥å…·ã€‚æ¶µç›–åŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€éŸ³é¢‘å¤„ç†ã€å¤šæ¨¡æ€ç­‰ä»»åŠ¡ã€‚


æ ¸å¿ƒæŠ½è±¡
- Auto classes ï¼ˆ`AutoModel`/`AutoTokenizer`/`AutoConfig` ç­‰ï¼‰ï¼šæ ¹æ®æ¨¡å‹ `id` è‡ªåŠ¨é€‰æ‹©æ­£ç¡®çš„æ¨¡å‹/åˆ†è¯å™¨ç±»ï¼Œæå¤§ç®€åŒ–è¿ç§»ä¸åŠ è½½ã€‚
- `Tokenizer`ï¼šæœ‰ Python æ…¢åˆ†è¯å™¨å’Œ Rust å¿«åˆ†è¯å™¨
- `pipeline` å‡½æ•°ï¼šä¸€è¡Œä»£ç åšå¸¸è§äººç‰©ï¼ˆæƒ…æ„Ÿåˆ†æã€å¡«ç©ºã€ç”Ÿæˆã€å›¾åƒåˆ†ç±»ã€è¯­éŸ³è¯†åˆ«ç­‰ï¼‰â€”â€”é€‚åˆå¿«é€ŸåŸå‹ä¸æ¼”ç¤ºï¼ˆè§ä¸‹ä¾‹ï¼‰ã€‚
```python
from transformers import pipeline

emodel_id = "meta-llama/Llama-3.2-3B-Instruct"
pipe = pipeline(
    "text-generation",
    emodel_id
)

messages = [{"role": "system", "content": "You are LLaMA, answer me in chinese"}]

while True:
    user_prompt = input("ğŸ˜Š ä½ è¯´ï¼š")
    if user_prompt.lower() == "exit":
        #print("èŠå¤©çµæŸå•¦ï¼Œä¸‹æ¬¡å†èŠå–”ï¼ğŸ‘‹")
        break
        
    messages.append({"role": "user", "content": user_prompt})
	outputs = pipe( # å‘¼å«æ¨¡å‹ç”Ÿæˆå›æ‡‰
		messages,
		max_new_tokens=2000,
		pad_token_id=pipe.tokenizer.eos_token_id
	)
    response = outputs[0]["generated_text"][-1]['content'] # ä»è¾“å‡ºä¸­å–å‡ºæ¨¡å‹ç”Ÿæˆçš„å›åº”

    print("ğŸ¤– åŠ©ç†è¯´ï¼š", response)
    messages.append({"role": "assistant", "content": response})
```


ç¤ºä¾‹ï¼šä½¿ç”¨ Hugging Face Transformers åº“è¿›è¡Œæ–‡æœ¬æƒ…ç»ªåˆ†æ
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

model_name = "distilbert-base-uncased-finetuned-sst-2-english"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

text = "I love programming!"

# é¢„å¤„ç†æ–‡æœ¬
tokens = tokenizer(text, padding=True, truncation=True, return_tensors="pt")

# æ¨¡å‹æ¨ç†
with torch.no_grad():
    outputs = model(**tokens)
    logits = outputs.logits
    probabilities = torch.softmax(logits, dim=1)
    
# è§£é‡Šç»“æœ
label_ids = torch.argmax(probabilities, dim=1)
labels = ['Negative', 'Positive']
label = labels[label_ids]
print(f"The sentiment is: {label}")
```


ç¤ºä¾‹ï¼š
```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "google/gemma-3-1b-it"   # æˆ–"meta-llama/Llama-3.2-3B-Instruct" or "meta-llama/Llama-3.2-1B-Instruct"

# å‡†å¤‡å¥½ tokenizer å’Œ model
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

print("Tokens number:", tokenizer.vocab_size) # æŸ¥çœ‹è¯­è¨€æ¨¡å‹æœ‰å¤šå°‘ Token å¯åœ¨â€œæ¥é¾™â€æ—¶é€‰æ‹©
print(tokenizer.decode([0, 1, 2, 3, 4, 5])) # æŸ¥çœ‹å¤šä¸ªç¼–å·å¯¹åº”çš„ token
# å°†æ‰€æœ‰ token ä¿å­˜è‡³æ–‡ä»¶
with open("tokens.txt", mode="a", encoding="utf-8") as f:
    for token_id in range(tokenizer.vocab_size): #token_id: [0, tokenizer.vocab_size-1]
    token = tokenizer.decode([token_id])
    f.write(f"<{token_id}:<{token}>\n")


# ä½¿ç”¨ tokenizer.encode ç¼–ç ä¸€æ®µæ–‡å­—
text = "å¤§å®¶å¥½"
tokens = tokenizer.encode(text, add_special_tokens=False) # æŠŠ text ä¸­çš„æ–‡å­—ç¼–ç æˆä¸€ä¸² token idï¼Œ add_special_tokens=False å¯ä»¥é¿å…åŠ ä¸Šä»£è¡¨èµ·å§‹çš„ç¬¦å·
print(text, "->", tokens)

# "good morning" å’Œ "i am good" ä¸­ good çš„ç¼–å·ä¸ä¸€æ ·ï¼ŒåŸå› æ˜¯ "good" å’Œ " good"ä¸æ˜¯åŒä¸€ä¸ª token
print("good morning" ,"->", tokenizer.encode("good morning",add_special_tokens=False))
print("i am good" ,"->", tokenizer.encode("i am good",add_special_tokens=False))

prompt = "1+1=" # è¾“å…¥æç¤ºè¯
input_ids = tokenizer.encode(prompt, return_tensors="pt") # "pt": PyTorch tensor æ ¼å¼
outputs = model(input_ids)
# outputs.logits æ˜¯æ¨¡å‹å¯¹è¾“å…¥æ¯ä¸ªä½ç½®ã€æ¯ä¸ª token çš„ä¿¡å¿ƒåˆ†æ•°ï¼ˆè¿˜æ²¡è½¬æˆæœºç‡ï¼‰
last_logits = outputs.logits[:, -1, :]  # å¾—åˆ°ä¸€ä¸ª token æ¥åœ¨ prompt åé¢çš„ä¿¡æ¯åˆ†æ•°
probabilities = torch.softmax(last_logits, dim=-1) # softmax å°†åŸå§‹ä¿¡å¿ƒåˆ†æ•°è½¬æˆ 0~1 ä¹‹é—´çš„æœºç‡å€¼

# æ‰“å°å‡ºæœºç‡æœ€é«˜çš„å‰ top_k å token
top_k = 10
top_p, top_indices = torch.topk(probabilities, top_k)
print(f"æœºç‡æœ€é«˜çš„å‰ {top_k} å token::")
for i in range(top_k):
    token_id = top_indices[0][i].item() # å–å¾—ç¬¬ i åçš„ token ID
    proability = top_p[0][i].item() # å¯¹åº”çš„æœºç‡
    token_str = tokenizer.decode(token_id) # å°† token ID è§£ç ä¸ºå­—ç¬¦ä¸²
    print(f"Token ID: {token_id}, Token: '{token_str}', æœºç‡: {probability:.4f}")
    
# è¿ç»­äº§ç”Ÿå¤šä¸ª token
prompt = "ä½ æ˜¯è°"
length = 20

for t in range(length):
    print("ç°åœ¨çš„ prompt æ˜¯:", prompt)
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    
    # ä½¿ç”¨æ¨¡å‹ model äº§ç”Ÿä¸‹ä¸€ä¸ª token
    outputs = model(input_ids)
    last_logits = outputs.logits[:, -1, :]
    probabilities = torch.softmax(last_logits, dim=-1)
    #top_p, top_indices = torch.topk(probabilities, 1)
    #token_id = top_indices[0][0].item() # å–å¾—ç¬¬ 1 åçš„ token IDï¼ˆå–æœºç‡æœ€é«˜çš„ tokenï¼‰
    token_id = torch.multinomial(probabilities, num_samples=1).squeeze() # æ¢æˆæ ¹æ®æ¦‚ç‡æ¥æ·éª°å­
    token_str = tokenizer.decode(token_id)
    print("ä¸‹ä¸€ä¸ª token æ˜¯:", token_str)
    
    prompt = prompt + token_str # å°†æ–°äº§ç”Ÿçš„ token æ¥å› promptï¼Œä½œä¸ºä¸‹ä¸€è½®çš„è¾“å…¥
    
# ä½¿ç”¨ model.generate åšæ–‡å­—æ¥é¾™
prompt = "ä½ æ˜¯è°?"
input_ids = tokenizer.encode(prompt, return_tensors="pt")

outputs = model.generate(
	input_ids,     # prompt çš„ token IDs
	max_length=20, # æœ€é•¿è¾“å‡ºçš„ token æ•°ï¼ˆåŒ…å«åŸæœ¬çš„ prompt)
	do_sample=True, # å¯ç”¨éšæœºæŠ½æ ·ï¼ˆä¸æ˜¯æ°¸è¿œæœºç‡æœ€é«˜ï¼‰
	top_k=3,      # æ¯æ¬¡åªä»æœºç‡æœ€é«˜çš„å‰ top-k ä¸ªä¸­æŠ½ï¼Œå¦‚æœ top_k = 1,å˜æˆé€‰æœºç‡æœ€é«˜çš„
	pad_token_id=tokenizer.eos_token_id,
	attention_mask=torch.ones_like(input_ids) 
)
# é™¤äº†è¿™é‡Œé‡‡ç”¨çš„åªä» top_k ä¸­é€‰æ‹©çš„æ–¹å¼å¤–ï¼Œè¿˜æœ‰è®¸å¤šæ ¹æ®æœºç‡é€‰æ‹© token çš„ç­–ç•¥
generated_text = tokenozer.decode(outputs[0]) # with skip_special_tokens=True è·³è¿‡ç‰¹æ®Š token
print("ç”Ÿæˆçš„æ–‡å­—æ˜¯:\n", generated_text)

# æ·»åŠ  chat æ¨¡æ¿ï¼Œåœ¨è¾“å…¥çš„ prompt å‰åŠ ä¸€äº›æ¨¡æ¿
prompt_with_chat_template = "ä½¿ç”¨è€…è¯´:" + prompt + "\nAIå›ç­”:" + "hahaha"
input_ids = tokenizer.encode(prompt_with_chat_template, return_tensors="pt")

# ä½¿ç”¨å®˜æ–¹çš„ chat æ¨¡æ¿
prompt = "ä½ æ˜¯èª°?"
print("ç¾åœ¨çš„ prompt æ˜¯:", prompt)
messages = [
    {"role": "system", "content": "ä½ çš„åå­—æ˜¯å°åŠ©æ‰‹"},  # è‡ªå·±æ·»åŠ  system prompt
    {"role": "user", "content": prompt},
]
print("ç¾åœ¨çš„ messages æ˜¯:", messages)
input_ids = tokenizer.apply_chat_template( #ä¸åªåŠ ä¸ŠChat Templateï¼Œé †ä¾¿å¹«ä½  encode äº†
    messages,
    add_generation_prompt=True,
    # add_generation_prompt=True è¡¨ç¤ºåœ¨æœ€å¾Œä¸€å€‹è¨Šæ¯å¾ŒåŠ ä¸Šä¸€å€‹ç‰¹æ®Šçš„ token (e.g., <|assistant|>)
    # é€™æœƒå‘Šè¨´æ¨¡å‹ç¾åœ¨è¼ªåˆ°å®ƒå›ç­”äº†ã€‚
    return_tensors="pt"
)
```

ç¤ºä¾‹ï¼šèŠå¤©ç¤ºä¾‹
```python
# å­˜æ”¾æ•´ä¸ªèŠå¤©å†å²ä¿¡æ¯çš„ list
messages = []

# ä¸€å¼€å§‹è®¾å®šè§’è‰²
messages.append({"role": "system", "content": "ä½ çš„åå­—æ˜¯ Llama, ç®€çŸ­å›ç­”é—®é¢˜"})

while True:
    # ç”¨æˆ·è¾“å…¥æ¶ˆæ¯
    user_prompt = input("Please input: ")
    
    if user_prompt.lower() == "exit":
        print("Bye~, see you next time!")
        break
    # å°†ç”¨æˆ·æ¶ˆæ¯åŠ è¿›å¯¹è¯è®°å½•    
    messages.append({"role": "user", "content": user_prompt})
    
    # å°†å†å²æ¶ˆæ¯è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ ¼å¼
    input_ids = tokenizer.apply_chat_template(
	    messages,
	    add_generation_prompt=True, # ä¼šåœ¨æ¶ˆæ¯åé—¨é¢åŠ å…¥ä¸€ä¸ªç‰¹æ®Šæ ‡è®°(<|assistant|>)
	    return_tensors="pt"
    )
    
    # ç”Ÿæˆæ¨¡å‹çš„é—®é¢˜
    outputs = model.generate(
	    input_ids,
	    max_length=2000,
	    do_sample=True,
	    top_k=3,
	    pad_token_id=tokenizer.eos_token_id,
	    attention_mask=torch.ones_like(input_ids)
    )
    
    # å°†æ¨¡å‹çš„è¾“å‡ºè½¬æ¢ä¸ºæ–‡å­—
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)
    
    # ä»ç”Ÿæˆçš„ç»“æœä¸­å–å‡ºæ¨¡å‹çœŸæ­£çš„å›å¤å†…å®¹ï¼ˆå»é™¤ç‰¹æ®Š tokenï¼‰
    response = generated_text.split("<|end_header_id|>")[-1].split("<|eot_id|>")[0].strip()
    
    print("ğŸ¤– åŠ©æ‰‹èªªï¼š", response)
    
    # å°†æ¨¡å‹å›å¤æ¶ˆæ¯åŠ è¿›å¯¹è¯è®°å½•
    messages.append({"role": "assistant", "content": response})
```